from elasticsearch import Elasticsearch, helpers
from datetime import datetime
from elasticsearch.helpers import BulkIndexError
import base64

es = Elasticsearch("http://localhost:9200", timeout=30)

def encode_file_to_base64(file_path):
    with open(file_path, "rb") as file:
        encoded_string = base64.b64encode(file.read()).decode('utf-8')
    return encoded_string

file_path = r"C:\Users\HP\Downloads\VolunteerUserManual (3).pdf"
file_base64 = encode_file_to_base64(file_path)
index_name = "questions"

if es.indices.exists(index=index_name):
    es.indices.delete(index=index_name)
    print(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù index Ø§Ù„Ù‚Ø¯ÙŠÙ…: {index_name}")

es.indices.create(
    index=index_name,
    body={
        "mappings": {
            "properties": {
                "title": {"type": "text"},
                "paragraph": {"type": "text"},
                "files": {"type": "text"},
                "file_base64": {"type": "text"},
                "keywords": {"type": "keyword"},
                "question_type": {"type": "keyword"},
                "changes": {"type": "text"},
                "related_questions": {
                    "type": "nested",
                    "properties": {
                        "q_id": {"type": "integer", "null_value": -1},
                        "q_title": {"type": "text"},
                        "type": {"type": "keyword"}
                    }
                },
                "category": {"type": "keyword"},
                "timestamp": {"type": "date"}
            }
        }
    }
)

print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ index: {index_name} Ù…Ø¹ mapping Ù…Ù†Ø§Ø³Ø¨")

raw_documents = [
    {
        "title": "Ù…Ø§ Ù‡ÙŠ Ø£Ø²Ù…Ø© Ø§Ù„Ù…ÙŠØ§Ù‡ ÙÙŠ Ø§Ù„Ø¹Ø±Ø§Ù‚ØŸ",
        "paragraph": "ÙŠØ¹Ø§Ù†ÙŠ Ø§Ù„Ø¹Ø±Ø§Ù‚ Ù…Ù† Ø´Ø­ Ù…ØªØ²Ø§ÙŠØ¯ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø§Ø¦ÙŠØ© Ø¨Ø³Ø¨Ø¨ Ù‚Ù„Ø© Ø§Ù„Ø£Ù…Ø·Ø§Ø± ÙˆØ³Ø¯ÙˆØ¯ Ø¯ÙˆÙ„ Ø§Ù„Ø¬ÙˆØ§Ø±.",
        "files": ["iraq_water_crisis.pdf"],
        "file_base64": file_base64,
        "keywords": ["Ù…ÙŠØ§Ù‡", "Ø§Ù„Ø¹Ø±Ø§Ù‚", "Ø´Ø­", "Ø£Ù†Ù‡Ø§Ø±"],
        "question_type": "Ø±Ø¦ÙŠØ³ÙŠ",
        "changes": "Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø¯ÙŠØ«Ø© Ø¹Ù† Ù†Ø³Ø¨ Ø§Ù„Ù…ÙŠØ§Ù‡",
        "related_questions": [
            "Ù…Ø§ Ù‡ÙŠ Ø£Ø³Ø¨Ø§Ø¨ Ø´Ø­ Ø§Ù„Ù…ÙŠØ§Ù‡ ÙÙŠ Ø§Ù„Ø¹Ø±Ø§Ù‚ØŸ",
            "Ù…Ø§ ØªØ£Ø«ÙŠØ± Ø³Ø¯ÙˆØ¯ ØªØ±ÙƒÙŠØ§ ÙˆØ¥ÙŠØ±Ø§Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø§Ù‚ØŸ"
        ],
        "category": "Ù…ÙˆØ§Ø±Ø¯ Ù…Ø§Ø¦ÙŠØ©"
    },
    {
        "title": "Ù…Ø§ Ø¯ÙˆØ± Ø³Ø¯ Ø§Ù„Ù…ÙˆØµÙ„ ÙÙŠ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ù…Ø§Ø¦ÙŠØŸ",
        "paragraph": "ÙŠØ¹Ø¯ Ø³Ø¯ Ø§Ù„Ù…ÙˆØµÙ„ Ø£Ø­Ø¯ Ø£ÙƒØ¨Ø± Ø§Ù„Ø³Ø¯ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¹Ø±Ø§Ù‚ ÙˆÙ„Ù‡ Ø¯ÙˆØ± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ ÙÙŠ Ø®Ø²Ù† Ø§Ù„Ù…ÙŠØ§Ù‡ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡.",
        "files": ["mosul_dam_info.pdf"],
        "file_base64": file_base64,
        "keywords": ["Ø³Ø¯", "Ø§Ù„Ù…ÙˆØµÙ„", "Ù…ÙŠØ§Ù‡", "Ø§Ù„Ø¹Ø±Ø§Ù‚"],
        "question_type": "ÙØ±Ø¹ÙŠ",
        "changes": "ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† ÙˆØ¶Ø¹ Ø§Ù„Ø³Ø¯",
        "related_questions": [
            "Ù…Ø§ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù„Ø§Ù†Ù‡ÙŠØ§Ø± Ø³Ø¯ Ø§Ù„Ù…ÙˆØµÙ„ØŸ"
        ],
        "category": "Ø¨Ù†ÙŠØ© ØªØ­ØªÙŠØ© Ù…Ø§Ø¦ÙŠØ©"
    }
]

existing_titles = set(doc["title"] for doc in raw_documents)
additional_docs = []

for doc in raw_documents:
    for rq_title in doc.get("related_questions", []):
        if rq_title not in existing_titles:
            additional_docs.append({
                "title": rq_title,
                "paragraph": "",  
                "files": [],
                "file_base64": "",
                "keywords": [],
                "question_type": "Ù…Ø±ØªØ¨Ø·",
                "changes": "ØªÙ…Øª Ø¥Ø¶Ø§ÙØªÙ‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙƒØ¹Ù†ØµØ± Ù…Ø±ØªØ¨Ø·",
                "related_questions": [],
                "category": ["ØºÙŠØ± Ù…ØµÙ†Ù"]
            })
            existing_titles.add(rq_title)

full_documents = raw_documents + additional_docs

title_to_meta = {
    doc["title"]: {"q_id": idx, "type": doc["question_type"]}
    for idx, doc in enumerate(full_documents)
}

documents = []
for idx, doc in enumerate(full_documents):
    related = []
    for rq_title in doc.get("related_questions", []):
        if rq_title in title_to_meta:
            related.append({
                "q_id": title_to_meta[rq_title]["q_id"],
                "q_title": rq_title,
                "type": title_to_meta[rq_title]["type"]
            })
    doc["related_questions"] = related
    doc["timestamp"] = datetime.now().isoformat()
    documents.append(doc)

actions = [
    {
        "_index": index_name,
        "_source": doc
    }
    for doc in documents
]

try:
    response = helpers.bulk(es, actions)
    print(f"âœ… ØªÙ… Ø¥Ø¯Ø®Ø§Ù„ {response[0]} Ù…Ø³ØªÙ†Ø¯(Ø§Øª) Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ index: {index_name}")
except BulkIndexError as e:
    print("âŒ Ø­Ø¯Ø«Øª Ø£Ø®Ø·Ø§Ø¡ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:")
    for error in e.errors:
        print(error)
